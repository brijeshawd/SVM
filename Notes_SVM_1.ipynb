{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Content\n",
        "\n",
        "- Geometric intuition\n",
        "\n",
        "- Hard Margin SVM"
      ],
      "metadata": {
        "id": "OOxzAPQUAIjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM\n",
        "- Popular in 2000's ( late 90s )\n",
        "- Kernel's SVM \n",
        "- Theoritically they are awesome. (but in practice better algorithms exists)\n",
        "- less frequent used now a days\n",
        "- Very challenging Maths."
      ],
      "metadata": {
        "id": "XmMk2eWvzoHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Geometric intution behind SVM**"
      ],
      "metadata": {
        "id": "i1I9nSCibTgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Lets understand the geometric intuition behind SVM through a simple example\n",
        "\n",
        "Supposedly we have a few +ve class datapoints and a few -ve class datapoints\n",
        "- Now we have two hyperplanes $\\pi_1$ and $\\pi_2$\n",
        "\n",
        "\n",
        "#### though both the hyperplanes $\\pi_1$ and $\\pi_2$ separates  the data into two distinct classes , which one better generalizes on the data ?\n",
        "\n",
        "- Intuitively, we can say hyperplane $\\pi_2$ does a better generalization on the data\n",
        "\n",
        "#### But how can we say $\\pi_2$ generalizes better on the data ?\n",
        "- if we look at the hyperplane $\\pi_1$\n",
        "- if we draw two hyperplanes parallel to $\\pi_1$ from the closest +ve datapoint and -ve datapoint to $\\pi_1$\n",
        "- we can see the gap or Margin between these two parallel drawn hyperplane\n",
        "\n",
        "Note: This Margin_1 is respective to hyperplane $\\pi_1$\n",
        "\n",
        "\n",
        "- if we look at the hyperplane $\\pi_2$\n",
        "- and draw the two hyperplanes parallel to $\\pi_2$ from the closest +ve datapoint and -ve datapoint to $\\pi_1$\n",
        "- we can see the gap or Margin between these two parallel drawn hyperplane( Margin_2) is much larger than Margin_1\n",
        "\n",
        "- Giving us the idea that we pick that hyperplane $\\pi$ that results in Largest Margin\n",
        "\n",
        "#### Why need Margin to be large ?\n",
        "- Intuitively . the larger the margin, the farther away the groups of datapoints are from one another which makes classification task much easy.\n",
        "\n",
        "\n",
        "Note: All Distance from datapoints are measured perpendicular to any Hyperplane $\\pi$\n",
        "- so we want the all the +ve and -ve datapoints to be as far as possible from the Hyperplane in order for it to generalize better on the data.\n",
        "\n",
        "Such classifiers which want the margin to be as large as possible is called Margin-maximising classifiers."
      ],
      "metadata": {
        "id": "NfLYwoEtVAg3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1xU6TygVDQErHVIzU7hxJ0O_Sx54YWYHd)\n"
      ],
      "metadata": {
        "id": "drW6MEh_JfDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Let's summarize everything**\n",
        "- We have some bunch of + ve and - ve points\n",
        "- Let's say $\\pi$ is margin maximizing hyperplanes,\n",
        "- $\\pi^+$ is the positive hyerplane parallel to $\\pi$ and touching the closest postive points to the $\\pi$\n",
        "- $\\pi^-$ is the positive hyerplane  parallel to $\\pi$ and touching the closest negative points to the $\\pi$\n",
        "- d is margin i.e, distance between  $\\pi^+$ and  $\\pi^-$ such that we want this margin to be as large as possible."
      ],
      "metadata": {
        "id": "xqZ1h8asLJbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1a80_hlyYgeIO4IGdkOnmy2DmYlp2Eic2)\n"
      ],
      "metadata": {
        "id": "EZBtLnghNFbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now if we recall from Regression models, we define a Hyperplane $\\pi$ as $w^Tx+b = 0 $\n",
        "Now lets assume that the parallel hyperplanes ($\\pi^+$ and $\\pi^-$) are defined as:\n",
        "- $w^Tx+b = 1 $\n",
        "- $w^Tx+b = -1 $\n",
        "\n",
        "#### Then what will be the length of the margin if $||w|| \\neq 1$ ?\n",
        "- recall from linear algebra lecture, we can define the Margin = $\\frac{2}{||w||}$\n",
        "\n",
        "#### Question: If data is in spiral shape, is the hyperplane suppose to be a straight line ?\n",
        "- No, We will be using a different variation of hyperplane\n",
        "- but for now we are seeing a simplest case where data is linearly separable"
      ],
      "metadata": {
        "id": "zjgubQwgO0wf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <img src='https://drive.google.com/uc?id=1lFpXDwSeBZ1haEmLePCpf-q3S-yE2JIx'/> \n"
      ],
      "metadata": {
        "id": "xN2FNxBuPNCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **How did we define margin = $\\frac{2}{||w||}$ ?**\n",
        "- we defined our hyperplane $\\pi^+$ and $\\pi^-$ as\n",
        "- $w^Tx+b = 1 $\n",
        "- $w^Tx+b = -1 $\n",
        "\n",
        "Now if we rearrange these equations we get,\n",
        "- $w^Tx+b -1 = 0 $\n",
        "- $w^Tx+b+1 = 0 $\n",
        "\n",
        "if we see from the origin,\n",
        "\n",
        "- the distance of hyperplane $\\pi^+$ can be defined as\n",
        "  - $\\frac{b-1}{||w||}$\n",
        "\n",
        "- the distance of hyperplane $\\pi^-$ can be defined as\n",
        "  - $\\frac{b+1}{||w||}$\n",
        "\n",
        "Hence the distance between the two hyperplanes will be the difference between $\\frac{b+1}{||w||}$ and $\\frac{b-1}{||w||}$ :\n",
        "- $d( \\pi^+,\\pi^-) = \\frac{2}{||w||}$"
      ],
      "metadata": {
        "id": "mJ8c2MSQRNlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1RspiVk65K543NhaxLIdLSM6Wdr2b9LLx'/>"
      ],
      "metadata": {
        "id": "FIiDte_0RRmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question***: we know that we want to maximize the margin or $\\frac{2}{||w||}$, so what will be the parameters for the margin ?\n",
        "---\n",
        "- Weight (w) and constant (b)\n",
        "\n",
        "## **Now what if we defined $\\pi^+$ as $w^Tx+b = k $ and $\\pi^-$ as $w^Tx+b = -k $ , will the margin change ?**\n",
        "\n",
        "Lets understand this with an example\n",
        "- Supposedly we take $ k = 10 $\n",
        "- So we intuitively want the separating hyperplane ($\\pi$) to be mid way between the positive datapoint hyperplane ($\\pi^+$) and negative datapoint hyperplane ($\\pi^-$) so as to have a better generalization\n",
        "- Now $\\pi^+ : w^Tx+b = 10 $ and $\\pi^-: w^Tx+b = -10$\n",
        "- Hence with the same concept of putting everything to LHS, we get our margin as :\n",
        "  - $\\frac{b+10}{||w||} - \\frac{b-10}{||w||} = \\frac{20}{||w||} $\n",
        "- Now we will maximize this margin on weight (w) and constant (b)\n",
        "\n",
        "\n",
        "Note: Since, the margin does not change if we take +1,-1 or k,-k ,\n",
        "- Hence for mathematical simplicity, We take +1, -1\n",
        "- Also +1, -1 corresponds to classes\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s7n0sQSqBQwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1QLs3YEbAosO4wvZNTdDpsmOxmeVdlbKX'/>"
      ],
      "metadata": {
        "id": "cHqr6dZFu3LP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hard Margin SVM**"
      ],
      "metadata": {
        "id": "Wis_i8wnbl24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we have some +ve datapoints and -ve datapoints with hyperplane $\\pi$ which separates them, and we have $\\pi^+$ and $\\pi^-$ as our parallel hyperplane\n",
        "- we see the setting of the example is a classification problem where we have n samples, where each sample contains some features x and a class label y which can take value +1 or -1\n",
        "\n",
        "Our aim is to maximize the margin , so how can we perform optimization here ?\n",
        "- lets look into our example to understand this\n",
        "What will be the value of the +ve datapoint which lies on the hyperplane $\\pi^+$ ?\n",
        "- 1 , since $\\pi^+: w^Tx+b=1$\n",
        "\n",
        "Now what will be the value of the +ve datapoints which lie beyond the hyperplane $\\pi^+$ ?\n",
        "- will have value greater than 1 , hence $\\pi^+: w^Tx+b > 1$\n",
        "\n",
        "Similarly, what will be the value of the -ve datapoints which lie beyond the hyperplane $\\pi^-$ ?\n",
        "- will have value less than -1 , hence $\\pi^+: w^Tx+b < -1$\n",
        "- so can we club both the cases and say,\n",
        "- for all n samples, our $(w^Tx_i + b)y_i \\geq 1 $\n",
        "\n",
        "How does $(w^Tx_i + b)y_i \\geq 1 $ works for both +ve and -ve datapoints ?\n",
        "- if we consider +ve datapoint,\n",
        "- $y_i $ will be 1\n",
        "- $(w^Tx_i + b)$ will be greater than equal to 1 , hence satisfied\n",
        "- if we consider -ve datapoints\n",
        "- - $y_i $ will be -1\n",
        "- $(w^Tx_i + b)$ will be also negative since it will be less than equal to 1 , hence (-ve) multiplied by (-ve) makes positive , hence the equation is satisfied here too\n",
        "\n",
        "\n",
        "- Now we can say , we maximize the margin such that for all n samples, our $(w^Tx_i + b)y_i \\geq 1 $\n",
        "\n",
        "What constraints does $(w^Tx_i + b)y_i \\geq 1 $ puts ?\n",
        "- all +ve datapoints should lie beyond $\\pi^+$\n",
        "- And all -ve datapoints should lie beyond $\\pi^-$\n",
        "\n",
        "Thus due to such constraints, it makes the hyperplane to have zero errors since it does not want any datatpoint to belong to the wrong side of the parallel hyperplanes. This makes the model a Hard-Margin classifier.\n",
        "\n",
        "\n",
        "Note: this selecting the best hyperplane for the linear with maximizing margin along with n constraints is Linear SVM which is the simplest version of SVM model\n",
        "\n"
      ],
      "metadata": {
        "id": "krbxUrvIDli4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1D0eSCz9S3dsfhn2XEhd2K0zoMUQmzQt0'/> \n"
      ],
      "metadata": {
        "id": "YmTbY38NX22_"
      }
    }
  ]
}