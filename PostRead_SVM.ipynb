{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Content\n",
        "\n",
        "- **Visual representation of SVM with different hyper-parameters**\n",
        "\n",
        "- **Radial Basis Kernel** \n",
        "\n",
        "- **PCA vs SVM**\n",
        "\n",
        "- **Support Vector Regression**"
      ],
      "metadata": {
        "id": "eHRAd1vX_D0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visual representation of SVM with different hyper-parameters"
      ],
      "metadata": {
        "id": "v9iHf7Zm-zPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "We can import SVM using sklearn.svm.SVC\n",
        "- Default C is 1.0\n",
        "- Default kernel is RBF\n",
        "- we can provide other kernels too like linear , polynomial(default degree 3) , or custom precomputed kernels\n",
        "\n",
        "Note: \n",
        "- Sklearn use $ \\gamma $, kernel coefficient for 'rbf'\n",
        "-  $ \\gamma $ is defined as inverse of $\\sigma $, $ \\ \\gamma = \\frac{1}{\\sigma} $\n",
        "\n",
        "Other parameters also available like coef0 ( b in our equation ),  class_weight, cache_size to speed up. You can study about them in detail [here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
        "\n",
        "This sklearn implementation is based on libsvm.\n"
      ],
      "metadata": {
        "id": "eYnizKP8Zup1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1VdBKs4Gq18emZKY63wvGWiJDnj_dBUj-'/> "
      ],
      "metadata": {
        "id": "6a1tvdgWWhq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Another interesting visualization you can see is [here](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py).\n",
        "\n",
        "If we look at input, we have two classes \"red\" and \"blue\"\n",
        "\n",
        " -  we can observe Nearest neigbor classification for the same input under Nearest neigbor column\n",
        " - If we observe Linear SVM, linear classifier here shades of color represents probablity point belong to that class\n"
      ],
      "metadata": {
        "id": "MIXc8ox_Wlv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1KU6es-WY7aoqUFAjeRDD68TmXErSjx64'/>"
      ],
      "metadata": {
        "id": "WL0ue6gUYGBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Can we observe anything by looking at SVM-RBF and KNN diagram ?\n",
        "- we can observe that SVM-RBF is very similar to KNN in region of data points, \n",
        "\n",
        "- but region where there is no data point they differ. \n",
        "\n"
      ],
      "metadata": {
        "id": "54OQ3WJiYGRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " <img width=80% src='https://drive.google.com/uc?id=1PlesIJpysszwBatoG63LFUD98T6k36Ri'/>"
      ],
      "metadata": {
        "id": "nZ1jGqbsYr9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In the region of no data points, for any test point $ x_q $ SVM-RBF will still predict but with a low probability.<br>\n",
        "\n",
        "Note: In case of decission trees and random forest, you will see bunch of axis-parallel hyperplanes\n"
      ],
      "metadata": {
        "id": "jUZVkiEbYqkk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  <img width=80% src='https://drive.google.com/uc?id=1ovNT5eFqpa_vY5BJux6RxgBm1h5SAjgL'/> \n"
      ],
      "metadata": {
        "id": "j2vfFihmWn7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question***: Can we incorporate any domain specific kernel code in sklearn ?\n",
        "---\n",
        "- Yes, you can create your own kernel and call it in sklearn code using callable option in \"kernel\" parameter\n",
        "- we can provide a precomputed similarity matrix or can provide a callable function\n",
        "\n",
        "Refer [this](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#:~:text=squared%20l2%20penalty.-,kernel,-%7B%E2%80%98linear%E2%80%99%2C%20%E2%80%98poly%E2%80%99%2C%20%E2%80%98rbf) for more details.\n",
        "\n"
      ],
      "metadata": {
        "id": "SNYvHzzefxB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1qxrM9_dp0uOs-CqV46VrwVP2zhKeuLKa'/>"
      ],
      "metadata": {
        "id": "E-o_wieTZxdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize effect of hyperparameter like <b>C and gamma </b>\n",
        "\n",
        "If we look at data, we have two classes \"red\" and \"blue\" \n",
        "\n",
        "#### How will decission surface change if $ \\gamma $ changes with constant C ?\n",
        "\n",
        "With fixed C, if we increase gamma ( $ \\sigma reduces ), model overfits\n",
        "- If we move horizontally in this image you can observe the effect of gamma ( &gamma; ) or &sigma;. \n",
        "- Model overfits with increasing gamma ( &gamma; ) or decreasing &sigma;.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_px0_BM2gTkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1cFlJkTuGcCbJXXwKqmx6Qu9JDf7JTZC6'/>"
      ],
      "metadata": {
        "id": "bXZZvWqOcGsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What can we observe if we fix $\\gamma $ and increase the value of C ?\n",
        " - initial at low value of C, we almost have linear classifer\n",
        " - With increasing C, model overfits\n",
        "\n"
      ],
      "metadata": {
        "id": "_rLan5ofcGzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " <img src='https://drive.google.com/uc?id=1YQo_yNR1EscGlStvsYyXRSf7Mzn7gg5y'/>"
      ],
      "metadata": {
        "id": "a0jmJOR9c3jj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- If we move vertical, we can see the effect of increasing C i.e. model overfits, \n",
        "- more complex non-linear boundaries.\n",
        "\n",
        "Note: we can observe that extent of overfitting is more in case of increasing $ \\gamma $ as compared to C"
      ],
      "metadata": {
        "id": "7r3sfThOc3oB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  <img width=80% src='https://drive.google.com/uc?id=17XPQYsTNnJbLyLcO7lETWC5HT8Hur4Y8'/> \n",
        "\n"
      ],
      "metadata": {
        "id": "W_CM9klZbVqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Radial Basis Kernel (RBF) "
      ],
      "metadata": {
        "id": "ma6kP0Pj_Q7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question: What does radial and basis means in Radial basis function ?\n",
        "\n",
        "Radial means \n",
        "- It's value depends only on the distance from origin, not the direction\n",
        "- Given origin, all the point which are equidistant from origin will have same value of Kernel \n",
        "\n",
        "Basis means,\n",
        " - it forms the basis for some function space of interest,\n",
        "\n",
        "Note: Recall basis definition : a set B of vectors in a vector space V is called a basis if every element of V may be written in a unique way as a finite linear combination of elements of B"
      ],
      "metadata": {
        "id": "w-qdPC3pFJdF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <img src='https://drive.google.com/uc?id=1UztuxCWQ-wLopniAJMJnUdTmz44AEkgd'/> \n"
      ],
      "metadata": {
        "id": "PQkBc3B9ckGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PCA vs SVM"
      ],
      "metadata": {
        "id": "LhgFgFuN_n_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question***:  In PCA we decrease dimension but here in SVM we are increasing, isn't is contradictory?\n",
        "---\n",
        "- In PCA, we project data from d dimension to d' dimensions by preserving as much variance as possible such that d' < d\n",
        "- But in SVM, using the kernel trick we go from d to d' such that d' > d. \n",
        "- In higher dimension it is easier to find seperating hyperplane.\n",
        "\n",
        "#### Aren't these two contradictory ?\n",
        "\n",
        "- No, because purpose of both the methods are different. \n",
        "\n",
        "Question: What is our objective in PCA and SVM ?\n",
        "\n",
        "- In PCA, our objective is  visualization or understanding data because it is easy to visualize data in low dimension, we can not easily interpret data more than 2 or 3 dimension.\n",
        "\n",
        "- On the other hand out task in SVM is classification i.e. to find best seperating hyperplane which is much easier in higher dimensions.\n",
        "\n",
        "- Like in logistic regression also, we introduced polynomial features so that we can find the seperator\n",
        "\n"
      ],
      "metadata": {
        "id": "fvkoz-7mhAoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1PaesPo6FfYR-Zgy25Bm0efz2E-uzDeAw'/>\n"
      ],
      "metadata": {
        "id": "E3agwx6ONDj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Regression ( SVR )"
      ],
      "metadata": {
        "id": "gBU8zes3K2PQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Can we do regression with SVM ?\n",
        "\n",
        "Ans: yes,\n",
        "\n",
        "lets understand it, with an example:\n",
        "\n",
        "- Imagine we have two features f1,f2 and we have some data points \n",
        "\n",
        "<br>\n",
        "\n",
        "The main intuition behind SVR is to find a best fitting line,  \n",
        "\n",
        "- Such that the maximum error (ϵ) between $y_i$ and $y_î = w^Tx_i + b$ \n",
        " - is as minimum as possible \n",
        "\n",
        "<br>\n",
        "\n",
        "Thus making the loss function as:\n",
        "- $ min_{(w,b)} \\  \\frac{1}{2}||w||^2 + C. ϵ $\n",
        "\n",
        "such that\n",
        "-  $ y_i - y_î  \\leq ϵ $\n",
        "- and also $y_î - y_i \\leq ϵ $ \n",
        " - with $ϵ \\geq 0 $\n",
        "\n",
        "\n",
        "**Note:** There is also version of SVR called kernalized SVR\n",
        "- SVR is not very popular, hence we tend on not using it much \n",
        "\n"
      ],
      "metadata": {
        "id": "tq7bO6iTU_Ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1T781Qh03bWv8ttvDTPK4cn5RlLllgeYU'/> "
      ],
      "metadata": {
        "id": "mNGk0V7l-42O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretability of SVM"
      ],
      "metadata": {
        "id": "JzNDa6n_K7-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### what all are the hyperparameters in SVM ?\n",
        "Ans: C and σ, as we have seen \n",
        "in the primal form of SVM:\n",
        "- $min_{w,b} \\frac{1}{2}||w|| + C. ∑_{i=1}^{n} ζ_i $\n",
        "\n",
        "- C becomes our hyperparameter, \n",
        " - which makes SVM overfit as it increases\n",
        "\n",
        "And we have also seen σ being a hyperparameter when using RBF kernel \n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "#### Assuming that we found the best $ \\sigma $ and best C, can we interpret SVM ?\n",
        "\n",
        "Ans: We can only draw the which datapoints can be Support vectors through non zero $α_i$\n",
        "\n",
        "- There is no native intepretation of SVM \n",
        "\n",
        "<br>\n",
        "\n",
        "But, if we think RBF-SVM as similar to KNN , \n",
        "\n",
        "- for any query point $x_q$, we can think of it as finding nearest neighbors.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note:** we can say RBF-SVM as similiar to KNN is a hacky way\n",
        "- of defining interpretability of SVM \n"
      ],
      "metadata": {
        "id": "L5S3kOPPXPCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=18Uefkx5ai-BrN-lb2KJlvxeS16bcUmhz'/> "
      ],
      "metadata": {
        "id": "QHHw460LUBv_"
      }
    }
  ]
}